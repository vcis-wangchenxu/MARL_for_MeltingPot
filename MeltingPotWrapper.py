import logging
logging.getLogger('absl').setLevel(logging.ERROR)

import numpy as np
import dm_env
from dm_env import specs
from typing import Dict, List, Any, Tuple
import pygame
import time

from meltingpot import substrate
from meltingpot import scenario

from meltingpot.configs import substrates as substrate_configs
from meltingpot.configs import scenarios as scenario_configs

class _SubstrateWrapper:
    """
    Wraps a Melting Pot Substrate environment.

        - Provides access to *all* agents
        - Requires an action dictionary containing actions for *all* agents
    """
    
    def __init__(self, substrate_name: str):
        print(f"[Wrapper] Initializing Substrate: {substrate_name}...")

        # 1. Load substrate config
        self.substrate_name = substrate_name                        # substrate name
        self._config = substrate.get_config(self.substrate_name)    # substrate config

        # 2. Get player roles and agent IDs
        self._roles = self._config.default_player_roles                # player roles
        self.n_agents = len(self._roles)                               # number of agents
        self.agent_ids = [f"player_{i}" for i in range(self.n_agents)] # agent IDs

        # 3. Get action space
        self._action_set = self._config.action_set                 # action set
        self.n_actions = len(self._action_set)                     # number of actions (discrete)

        # 4. Build the underlying Melting Pot environment
        self._env = substrate.build_from_config(self._config, roles=self._roles)

        # 5. Get episode length limit
        lab2d_settings = self._config.lab2d_settings_builder(     # get lab2d settings
            roles=self._roles, config=self._config
        )
        self.episode_limit = lab2d_settings.get("maxEpisodeLengthFrames", 1000) # episode limit
        self._step_count = 0

        # 6. Get observation space (specifically RGB and WORLD.RGB)
        self._obs_spec = self._config.timestep_spec.observation["RGB"] 
        self._render_spec = self._config.timestep_spec.observation["WORLD.RGB"]
        
        self.obs_shape = (
            self._obs_spec.shape[2], 
            self._obs_spec.shape[0], 
            self._obs_spec.shape[1]
        )
        self._obs_size = np.prod(self.obs_shape)            # single agent obs size (H * W * C)
        self.state_shape = self.n_agents * self._obs_size   # global state size (n_agents * H * W * C)

        self.render_shape = self._render_spec.shape         # (H, W, C)

        self._last_timestep = None
        print(f"[Wrapper] Substrate initialization complete. N_Agents = {self.n_agents}")

    def get_env_info(self) -> dict:
        info = {
            "n_agents": self.n_agents,              # number of agents
            "n_actions": self.n_actions,            # number of actions
            "agent_ids": self.agent_ids,            # agent IDs
            "obs_shape": self.obs_shape,            # observation shape
            "state_shape": self.state_shape,        # state shape
            "render_shape": self.render_shape,      # render shape
            "episode_limit": self.episode_limit      # episode limit
        }
        return info

    def _preprocess_obs(self, obs_dict: dict) -> np.ndarray:
        """ Preprocess observation dictionary:
            - Convert 'RGB' to (C, H, W) and normalize
        """
        processed_obs = {}            # dictionary to hold processed observations                                  
        if 'RGB' in obs_dict:         # process RGB
            obs_rgb = obs_dict['RGB'].astype(np.float32)  
            obs_rgb /= 255.0                              
            processed_obs['RGB'] = np.transpose(obs_rgb, (2, 0, 1))   
        
        # if 'COLLECTIVE_REWARD' in obs_dict:  # retain collective reward if exists
        #     processed_obs['COLLECTIVE_REWARD'] = obs_dict['COLLECTIVE_REWARD']
        
        return processed_obs

    def get_obs(self) -> Dict[str, np.ndarray]:
        """ Get observations for all agents as a dictionary. """
        if self._last_timestep is None:
            raise RuntimeError("Must call reset() before calling get_obs()")
            
        agent_observations_list = self._last_timestep.observation  # list of observations for all agents
        
        obs_dict = {}                                    # dictionary to hold processed observations
        for i, agent_id in enumerate(self.agent_ids):    # iterate over agents
            obs_dict[agent_id] = self._preprocess_obs(
                agent_observations_list[i]
            )
        return obs_dict                                  # return the observation dictionary

    def get_state(self) -> np.ndarray:
        """ Get global state (flattened concatenation of all 'RGB' observations). """
        obs_dict = self.get_obs()   # get observations dictionary
        # Extract only the RGB parts to construct the global state
        obs_list = [obs_dict[agent_id]['RGB'] for agent_id in self.agent_ids if 'RGB' in obs_dict[agent_id]]
        if not obs_list:            # if obs_list is empty
            return np.array([])  
            
        obs_stack = np.stack(obs_list, axis=0)    # stack along new axis
        state = obs_stack.flatten()               # flatten to 1D array
        return state                                                

    def reset(self) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
        """ Reset the environment and return initial observations and state. """
        self._step_count = 0
        self._last_timestep = self._env.reset()
        return self.get_obs(), self.get_state()

    def step(self, actions_dict: Dict[str, int]) -> Tuple[Dict, Dict, Dict, Dict, Dict]:
        """ Take a step in the environment using the provided actions dictionary. """
        actions_list = [actions_dict[agent_id] for agent_id in self.agent_ids] # convert dict to list
        self._last_timestep = self._env.step(actions_list)                     # take a step
        self._step_count += 1                                                  # increment step count
        
        rewards_dict = {}                                 # dictionary for rewards
        dones_dict = {}                                   # dictionary for done flags

        agent_rewards_list = self._last_timestep.reward   # list of rewards for all agents
        
        global_done = False
        if self._step_count >= self.episode_limit:        # check episode limit
            global_done = True
        elif self._last_timestep.last():                  # check if episode ended
            global_done = True

        for i, agent_id in enumerate(self.agent_ids):
            rewards_dict[agent_id] = agent_rewards_list[i]  # assign rewards
            dones_dict[agent_id] = global_done              # assign done flags
            
        next_obs_dict = self.get_obs()                      # get next observations
        next_state = self.get_state()                       # get next state
        info_dict = {}                                    # empty info dictionary

        if self._last_timestep.observation:                                             # if observations exist
            raw_obs_agent_0 = self._last_timestep.observation[0]                        # raw obs of agent 0
            if 'COLLECTIVE_REWARD' in raw_obs_agent_0:
                info_dict['collective_reward'] = raw_obs_agent_0['COLLECTIVE_REWARD']    # add collective reward to info
        
        return next_obs_dict, next_state, rewards_dict, dones_dict, info_dict

    def render(self) -> np.ndarray:
        """ Render the current environment state as an RGB array. """
        if self._last_timestep is None:
            raise RuntimeError("Must call reset() before calling render()")
        obs_list = self._env.observation()
        if not obs_list:
            raise ValueError("self._env.observation() returned an empty list.")
        obs_dict = obs_list[0]           # we get WORLD.RGB from the first element (dict) of the list

        if 'WORLD.RGB' not in obs_dict:
            raise KeyError("Could not find 'WORLD.RGB' in self._env.observation()[0].")

        return obs_dict['WORLD.RGB']
    
    def close(self):
        self._env.close()

class _ScenarioWrapper:
    """
    Encapsulates a Melting Pot Scenario environment.

    - Only exposes *focal* agents (the learning agents)
    - Requires an action dictionary containing only *focal* agent actions  
    - Handles bot actions automatically internally
    - Maintains API compatibility with _SubstrateWrapper
    """
    
    def __init__(self, scenario_name: str):
        print(f"[Wrapper] Initializing Scenario: {scenario_name}...")
        self.scenario_name = scenario_name

        # 1. Load scenario config (needed to get is_focal etc.)
        self._config = scenario.get_config(self.scenario_name)

        # 2. Build the underlying Melting Pot *scenario* environment
        self._env = scenario.build(self.scenario_name)

        # 3. Identify *focal* agents
        self._roles = self._config.roles                   # player roles
        self._is_focal = self._config.is_focal             # focal agent flags
        
        self.agent_ids = []                                # list of focal agent IDs
        self._focal_agent_indices = []                     # indices of focal agents in the full env
        for i, is_focal in enumerate(self._is_focal):      # iterate over agents
            if is_focal:
                self.agent_ids.append(f"player_{i}")       # add focal agent ID
                self._focal_agent_indices.append(i)        # add focal agent index
                
        self.n_agents = len(self.agent_ids)                # number of focal agents

        # 4. Get action and observation spaces (only for focal agents)
        self._action_spec_list = self._env.action_spec()        # list of action specs
        self._obs_spec_list = self._env.observation_spec()      # list of observation specs
        
        self.n_actions = self._action_spec_list[0].num_values   # assume all focal agents have same action space
        
        obs_spec_rgb = self._obs_spec_list[0]["RGB"]       # assume all focal agents have same obs space
        self.obs_shape = (
            obs_spec_rgb.shape[2], 
            obs_spec_rgb.shape[0], 
            obs_spec_rgb.shape[1]
        )
        self._obs_size = np.prod(self.obs_shape)            # single focal agent obs size (H * W * C)
        self.state_shape = self.n_agents * self._obs_size   # global state size (n_focal_agents * H * W * C)

        # 5. Get episode length limit
        self._substrate_config = substrate.get_config(self._config.substrate)   # substrate config
        self._substrate_roles = self._substrate_config.default_player_roles     # substrate roles
        
        lab2d_settings = self._substrate_config.lab2d_settings_builder(         # get lab2d settings
            roles=self._substrate_roles, config=self._substrate_config
        )
        self.episode_limit = lab2d_settings.get("maxEpisodeLengthFrames", 1000)
        self._step_count = 0

        self._render_spec = self._substrate_config.timestep_spec.observation["WORLD.RGB"]
        self.render_shape = self._render_spec.shape         # (H, W, C)

        self._last_timestep = None
        print(f"[Wrapper] Scenario initialization finished. N_Agents = {self.n_agents} (focal agents only)")

    def get_env_info(self) -> dict:
        info = {
            "n_agents": self.n_agents,              # number of agents
            "n_actions": self.n_actions,            # number of actions
            "agent_ids": self.agent_ids,            # agent IDs
            "obs_shape": self.obs_shape,            # observation shape
            "state_shape": self.state_shape,        # state shape
            "render_shape": self.render_shape,      # render shape
            "episode_limit": self.episode_limit    # episode limit
        }
        return info

    def _preprocess_obs(self, obs_dict: dict) -> np.ndarray:
        """
        Preprocessing the observation dictionary:
        - Transform 'RGB' to (C, H, W) format and normalize
        - Retain 'COLLECTIVE_REWARD' unchanged if it exists
        """
        processed_obs = {}

        # 1. Process 'RGB' observation
        if 'RGB' in obs_dict:      # process RGB
            obs_rgb = obs_dict['RGB'].astype(np.float32)
            obs_rgb /= 255.0   
            processed_obs['RGB'] = np.transpose(obs_rgb, (2, 0, 1))
        
        # # 2. Retain 'COLLECTIVE_REWARD' if present
        # if 'COLLECTIVE_REWARD' in obs_dict:
        #     processed_obs['COLLECTIVE_REWARD'] = obs_dict['COLLECTIVE_REWARD']
            
        return processed_obs

    def get_obs(self) -> Dict[str, np.ndarray]:
        if self._last_timestep is None:
            raise RuntimeError("Must call reset() before calling get_obs()")
            
        focal_obs_list = self._last_timestep.observation  # list of observations for all agents
        
        obs_dict = {}                                     # dictionary to hold processed observations
        for i, agent_id in enumerate(self.agent_ids):     # iterate over focal agents
            obs_dict[agent_id] = self._preprocess_obs(    # preprocess observation
                focal_obs_list[i]
            )
        return obs_dict

    def get_state(self) -> np.ndarray:
        """ Get global state (flattened concatenation of all 'RGB' observations of focal agents). """
        obs_dict = self.get_obs()    # get observations dictionary
        # Extract only the RGB parts to construct the global state
        obs_list = [obs_dict[agent_id]['RGB'] for agent_id in self.agent_ids if 'RGB' in obs_dict[agent_id]]
        if not obs_list:
            return np.array([]) 

        obs_stack = np.stack(obs_list, axis=0)  # stack along new axis
        state = obs_stack.flatten()             # flatten to 1D array
        return state

    def reset(self) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
        """ Reset the environment and return initial observations and state. """
        self._step_count = 0
        self._last_timestep = self._env.reset()
        return self.get_obs(), self.get_state()

    def step(self, actions_dict: Dict[str, int]) -> Tuple[Dict, Dict, Dict, Dict, Dict]:
        """ Take a step in the environment using the provided actions dictionary for focal agents. """
        actions_list = [actions_dict[agent_id] for agent_id in self.agent_ids]  # convert dict to list for focal agents
        self._last_timestep = self._env.step(actions_list)                      # take a step
        self._step_count += 1
        
        rewards_dict = {}           # dictionary for rewards
        dones_dict = {}             # dictionary for done flags

        focal_rewards_list = self._last_timestep.reward    # list of rewards for focal agents
        
        global_done = False
        if self._step_count >= self.episode_limit:         # check episode limit
            global_done = True
        elif self._last_timestep.last():                   # check if episode ended
            global_done = True

        for i, agent_id in enumerate(self.agent_ids):       # iterate over focal agents
            rewards_dict[agent_id] = focal_rewards_list[i]
            dones_dict[agent_id] = global_done
            
        next_obs_dict = self.get_obs()                      # get next observations
        next_state = self.get_state()                       # get next state
        info_dict = {}                                      # empty info dictionary

        if self._last_timestep.observation:
            raw_obs_agent_0 = self._last_timestep.observation[0]
            if 'COLLECTIVE_REWARD' in raw_obs_agent_0:
                info_dict['collective_reward'] = raw_obs_agent_0['COLLECTIVE_REWARD']
        
        return next_obs_dict, next_state, rewards_dict, dones_dict, info_dict
    
    def render(self) -> np.ndarray:
        """ Render the current environment state as an RGB array. """
        if self._last_timestep is None:
            raise RuntimeError("Must call reset() before calling render()")

        # self._env._substrate.observation() returns a list of observation dictionaries
        obs_list = self._env._substrate.observation()
        if not obs_list:
            raise ValueError("self._env._substrate.observation() returned an empty list.")

        obs_dict = obs_list[0]           # we get WORLD.RGB from the first element (dict) of the list
        if 'WORLD.RGB' not in obs_dict:
            raise KeyError("Could not find 'WORLD.RGB' in self._env._substrate.observation()[0].")

        return obs_dict['WORLD.RGB']

    def close(self):
        self._env.close()

def build_meltingpot_env(env_name: str):
    """
    Constructs a Melting Pot environment (either Substrate or Scenario)
    and returns a wrapper providing a unified API interface.
    """

    # Check if env_name is in the imported SUBSTRATES collection
    if env_name in substrate_configs.SUBSTRATES:
        return _SubstrateWrapper(substrate_name=env_name)

    # Check if env_name is in the imported SCENARIO_CONFIGS dictionary
    elif env_name in scenario_configs.SCENARIO_CONFIGS:
        return _ScenarioWrapper(scenario_name=env_name)

    # Otherwise, raise an error
    else:
        raise ValueError(
            f"Unknown Melting Pot environment name: '{env_name}'.\n"
            f"It is neither in meltingpot.configs.substrates.SUBSTRATES nor in meltingpot.configs.scenarios.SCENARIO_CONFIGS."
        )

def run_pygame_test(env, test_name: str, scale_factor: int = 10, fps: int = 10, steps: int = 100):
    """
    Run a brief test loop using Pygame.

    Args:
        env: An initialized _SubstrateWrapper or _ScenarioWrapper instance
        test_name: Title for the display window ("Substrate" or "Scenario")
        scale_factor: Factor to scale up the images (original (40, 72) is too small)
        fps: Rendering frame rate
        steps: Total number of steps to execute
    """
    
    print(f"\n--- Launching Pygame test: {test_name} ---")
    print(f"--- Running {steps} steps... (Press 'Q' or close the window to exit) ---")

    pygame.init()

    # Get the correct render shape (H, W, C) from env_info
    render_shape = env.get_env_info()['render_shape']
    H, W, C = render_shape

    # Pygame uses (Width, Height)
    screen_size = (W * scale_factor, H * scale_factor)
    screen = pygame.display.set_mode(screen_size)
    pygame.display.set_caption(f"Test: {test_name}")
    clock = pygame.time.Clock()

    # Reset the environment
    obs_dict, state = env.reset()
    
    running = True
    for i in range(steps):
        if not running:
            break

        # 1. Handle quit events
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False
            if event.type == pygame.KEYDOWN:
                if event.key == pygame.K_q:
                    running = False

        # 2. Get the render frame
        frame = env.render() # (H, W, C)

        # 3. Prepare Pygame Surface
        # Pygame surface expects (W, H, C), so we swap H and W axes
        frame_transposed = frame.swapaxes(0, 1) # (W, H, C)
        surface = pygame.surfarray.make_surface(frame_transposed)

        # 4. Scale and draw to the screen
        scaled_surface = pygame.transform.scale(surface, screen_size)
        screen.blit(scaled_surface, (0, 0))
        pygame.display.flip()

        # 5. Generate random actions
        # Pygame test does not need to access obs or rewards, we only test rendering
        actions_dict = {
            agent_id: np.random.randint(0, env.n_actions) 
            for agent_id in env.agent_ids
        }

        # 6. Execute one step
        next_obs_dict, next_state, rewards_dict, dones_dict, info = env.step(actions_dict)

        # 7. Print collective reward (if available)
        if 'COLLECTIVE_REWARD' in next_obs_dict[env.agent_ids[0]]:
            coll_reward = info['collective_reward']
            print(f"  [Step {i+1}/{steps}] {test_name} - Collective Reward: {coll_reward}", end="\r")

        # 8. Control frame rate
        clock.tick(fps)
        
        if dones_dict[env.agent_ids[0]]:
            print(f"\n  [Step {i+1}] Environment terminated, resetting...")
            obs_dict, state = env.reset()

    pygame.quit()
    print(f"\n--- Pygame test: {test_name} completed ---")

if __name__ == "__main__":

    # --- Global Settings ---

    # !!! Set to True to pop up the Pygame window for testing !!!
    SHOW_RENDER_WITH_PYGAME = True

    # (40, 72, 3) is the correct WORLD.RGB shape for collaborative_cooking
    CORRECT_RENDER_SHAPE = (40, 72, 3)
    PYGAME_SCALE = 10  # Scale 40x72 to 400x720
    PYGAME_FPS = 10    # Simulate running speed
    PYGAME_STEPS = 200 # Simulate running steps

    # --- Test 1: Load Substrate (Control All Agents) ---
    print("="*50)
    print("Test 1: Load Substrate (collaborative_cooking__asymmetric)")
    print("="*50)
    
    env_substrate = build_meltingpot_env("collaborative_cooking__asymmetric")
    
    info_sub = env_substrate.get_env_info()
    print("Substrate environment info (get_env_info()):")
    print(f"  - n_agents: {info_sub['n_agents']}")
    print(f"  - agent_ids: {info_sub['agent_ids']}")
    print(f"  - render_shape: {info_sub['render_shape']}")

    # Validate metadata
    assert info_sub["n_agents"] == 2
    assert info_sub["agent_ids"] == ["player_0", "player_1"]
    assert info_sub["render_shape"] == CORRECT_RENDER_SHAPE 

    # Run Pygame test (if enabled)
    if SHOW_RENDER_WITH_PYGAME:
        run_pygame_test(
            env_substrate, 
            test_name="Substrate (All Agents)", 
            scale_factor=PYGAME_SCALE, 
            fps=PYGAME_FPS,
            steps=PYGAME_STEPS
        )
    
    # Run headless test (check API)
    obs_dict, state = env_substrate.reset()
    frame = env_substrate.render()
    assert frame.shape == CORRECT_RENDER_SHAPE
    actions_dict_sub = {
        "player_0": np.random.randint(0, info_sub["n_actions"]),
        "player_1": np.random.randint(0, info_sub["n_actions"])
    }
    next_obs_dict, _, rewards_sub, _, info_sub_step = env_substrate.step(actions_dict_sub)
    assert "COLLECTIVE_REWARD" not in next_obs_dict["player_0"]
    assert "collective_reward" in info_sub_step
    
    env_substrate.close()
    print("\n✅ Substrate validation successful!")


    # --- Test 2: Load Scenario (Control Focal Agent Only) ---
    print("\n" + "="*50)
    print("Test 2: Load Scenario (collaborative_cooking__asymmetric_0)")
    print("="*50)

    env_scenario = build_meltingpot_env("collaborative_cooking__asymmetric_0")

    info_scn = env_scenario.get_env_info()
    print("Scenario environment info (get_env_info()):")
    print(f"  - n_agents: {info_scn['n_agents']}")
    print(f"  - agent_ids: {info_scn['agent_ids']}")
    print(f"  - render_shape: {info_scn['render_shape']}")

    # Validate metadata
    assert info_scn["n_agents"] == 1
    assert info_scn["agent_ids"] == ["player_0"]
    assert info_scn["render_shape"] == CORRECT_RENDER_SHAPE # <-- Fixed

    # Run Pygame test (if enabled)
    if SHOW_RENDER_WITH_PYGAME:
        run_pygame_test(
            env_scenario, 
            test_name="Scenario (Focal Agents + Bots)", 
            scale_factor=PYGAME_SCALE, 
            fps=PYGAME_FPS,
            steps=PYGAME_STEPS
        )

    # Run headless test (check API)
    obs_dict, state = env_scenario.reset()
    frame_scn = env_scenario.render()
    assert frame_scn.shape == CORRECT_RENDER_SHAPE
    actions_dict_scn = {
        "player_0": np.random.randint(0, info_scn["n_actions"])
    }
    next_obs_dict_scn, _, rewards_scn, _, info_scn_step = env_scenario.step(actions_dict_scn)
    assert "player_0" in rewards_scn
    assert "player_1" not in rewards_scn
    assert "COLLECTIVE_REWARD" not in next_obs_dict_scn["player_0"]
    assert "collective_reward" in info_scn_step

    env_scenario.close()
    print("\n✅ Scenario validation successful!")
    
    print("\n" + "="*50)
    print("🎉 Factory function (build_meltingpot_env) compatibility validation passed! 🎉")
    print("="*50)

