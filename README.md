# MARL for Melting Pot

An implementation of Multi-Agent Reinforcement Learning (MARL) algorithms, specifically **MAPPO (Multi-Agent PPO)** and **VDN (Value Decomposition Networks)**, designed for the [DeepMind Melting Pot](https://github.com/google-deepmind/meltingpot) benchmark.

## ğŸš€ Features

*   **Algorithms**: 
    *   **MAPPO** (On-Policy): Cooperative PPO with Centralized Value Function.
    *   **VDN** (Off-Policy): Value Decomposition Networks independent Q-learning with shared rewards.
*   **Environment**: Full support for DeepMind Melting Pot substrates (e.g., `clean_up`, `collaborative_cooking`).
*   **Vectorization**: Efficient `MeltingPotAsyncVectorEnv` for parallel environment rollouts.
*   **Architectures**: 
    *   **CNN + RNN**: Handles partial observability and image inputs.
    *   **PopArt / Normalization**: Input normalization for stable training.
*   **Logging**: Integrated with **[SwanLab](https://swanlab.cn)** for experiment tracking and visualization.

## ğŸ“‚ Project Structure

```
marl_for_meltingpot/
â”œâ”€â”€ algorithms/             # Algorithm implementations
â”‚   â”œâ”€â”€ mappo.py            # MAPPO (PPO with Centralized Critic)
â”‚   â””â”€â”€ vdn.py              # VDN (Value Decomposition Networks)
â”œâ”€â”€ configs/                # Configuration files
â”‚   â”œâ”€â”€ mappo_meltingpot.yaml
â”‚   â””â”€â”€ vdn_meltingpot.yaml
â”œâ”€â”€ envs/                   # Environment wrappers
â”‚   â”œâ”€â”€ MeltingPotWrapper.py   # Gym-like wrapper for Melting Pot
â”‚   â””â”€â”€ multi_envs.py          # Multiprocessing VectorEnv
â”œâ”€â”€ memories/               # Experience Replay Buffers
â”‚   â”œâ”€â”€ ReplayBuffer.py     # For Off-Policy (VDN)
â”‚   â””â”€â”€ RolloutBuffer.py    # For On-Policy (MAPPO)
â”œâ”€â”€ networks/               # Neural Network Architectures
â”‚   â”œâ”€â”€ DRQN.py             # Recurrent Q-Network
â”‚   â””â”€â”€ MAPPO_Network.py    # Actor-Critic Networks
â”œâ”€â”€ results/                # Training outputs (models, logs)
â”œâ”€â”€ utils/                  # Utility functions
â”‚   â”œâ”€â”€ config.py           # Config loader
â”‚   â”œâ”€â”€ evaluator.py        # Evaluation logic
â”‚   â””â”€â”€ util.py             # Seeding and plotting
â”œâ”€â”€ run.py                  # Main entry point for training
â”œâ”€â”€ train_offpolicy.py      # Training loop for Off-Policy
â””â”€â”€ train_onpolicy.py       # Training loop for On-Policy
```

## ğŸ› ï¸ Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/your-username/marl_for_meltingpot.git
    cd marl_for_meltingpot
    ```

2.  **Install Dependencies:**
    Ensure you have Python 3.8+ and PyTorch installed. You also need DeepMind Melting Pot.
    ```bash
    pip install torch numpy pygame pyyaml tqdm pandas matplotlib seaborn dm-meltingpot
    pip install swanlab  # For logging
    ```

## ğŸƒ Usage

### Training

You can train agents using the `run.py` script. You must specify a configuration file using the `--config` argument.

**Train MAPPO (On-Policy):**
```bash
python run.py --config configs/mappo_meltingpot.yaml
```

**Train VDN (Off-Policy):**
```bash
python run.py --config configs/vdn_meltingpot.yaml
```

### Configuration (`.yaml`)
Modify the files in `configs/` to adjust hyperparameters.
Key parameters:
*   `env`: The Melting Pot substrate name (e.g., `clean_up`).
*   `algo`: Path to algorithm class (e.g., `algorithms.mappo.MAPPO`).
*   `num_envs`: Number of parallel environments for data collection.
*   `share_parameters`: Whether agents share weights (True/False).

### Evaluation
Evaluation runs automatically during training based on the `eval_freq` parameter in the config.
*   Models are saved in `results/<experiment_name>/models/`.
*   Best models are saved as `model_best.pth`.

## ğŸ“Š Logging & Visualization

This project uses **SwanLab** for logging metrics (Reward, Loss, Episode Length).
*   Logs are saved in `results/logs/`.
*   You can view training curves in the cloud if SwanLab is configured, or locally.

## ğŸ¤– Algorithms Details

*   **MAPPO**: Implements PPO with a centralized value function. It uses a CNN encoder for visual observations and an optional vector encoder for other data. It supports Recurrent Neural Networks (GRU) to handle memory.
*   **VDN**: Implements Value Decomposition Networks. It approximates the joint Q-value as the sum of individual local Q-values. It uses a DRQN (Deep Recurrent Q-Network) architecture.

## ğŸ“ Notes
*   **Global State**: The environment wrapper automatically attempts to extract `WORLD.RGB` for centralized critics if available in the substrate.
*   **Vector Observations**: The wrapper automatically flattens and concatenates all vector observations defined in the Melting Pot spec, excluding RGB.

## ğŸ“„ License
MIT License
